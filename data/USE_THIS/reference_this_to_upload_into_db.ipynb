{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient import discovery\n",
    "import re\n",
    "import time\n",
    "import sys\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from multiprocessing import Pool, cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df886fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_format(path):\n",
    "\n",
    "    # connect to sql database\n",
    "    conn = sqlite3.connect(path)\n",
    "\n",
    "    # load a data sample\n",
    "    query = \"SELECT * FROM comments WHERE body is NOT '' AND body is NOT NULL\"\n",
    "\n",
    "    reddit_df = pd.read_sql_query(query, conn)\n",
    "    reddit_df = reddit_df[reddit_df['body'].notna()].reset_index(drop=False)\n",
    "\n",
    "    # check if Toxicity is part of the schema\n",
    "    if \"Toxicity\" in reddit_df.columns:\n",
    "        pass\n",
    "    else:\n",
    "        # if not, add all the relevant values\n",
    "        conn.execute(\"ALTER TABLE comments ADD Toxicity real;\")\n",
    "        conn.execute(\"ALTER TABLE comments ADD Insult real;\")\n",
    "        conn.execute(\"ALTER TABLE comments ADD Severe_Toxicity real;\")\n",
    "        conn.execute(\"ALTER TABLE comments ADD Identity_Attack real;\")\n",
    "        conn.execute(\"ALTER TABLE comments ADD Profanity real;\")\n",
    "\n",
    "        conn.commit()\n",
    "    \n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceea858f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_googlePerspective(path, client, sample):\n",
    "\n",
    "    conn = sqlite3.connect(path)\n",
    "\n",
    "    #print(sample)\n",
    "\n",
    "    sample_text = sample['body']\n",
    "\n",
    "    # check if the submissions text is within limit for the API\n",
    "\n",
    "    # if it is larger, then break it down\n",
    "    if len(sample_text.encode('utf-8')) > 20480:\n",
    "        try:\n",
    "            # break it in two:\n",
    "\n",
    "            # set two headers\n",
    "            response_header_1 = {\n",
    "    'comment': { 'text': sample_text[:20000]},\n",
    "    'requestedAttributes': {'TOXICITY': {}, 'INSULT':{}, 'THREAT':{}, 'SEVERE_TOXICITY': {}, 'IDENTITY_ATTACK': {}, 'PROFANITY': {}}\n",
    "    }       \n",
    "            response_header_2 = {\n",
    "    'comment': { 'text': sample_text[20000:]},\n",
    "    'requestedAttributes': {'TOXICITY': {}, 'INSULT':{}, 'THREAT':{}, 'SEVERE_TOXICITY': {}, 'IDENTITY_ATTACK': {}, 'PROFANITY': {}}\n",
    "    }\n",
    "            \n",
    "            # get two responses\n",
    "\n",
    "            response_1 = client.comments().analyze(body=response_header_1).execute()\n",
    "            response_2 = client.comments().analyze(body=response_header_2).execute()\n",
    "            \n",
    "            # add their average response\n",
    "            sample['Toxicity'] = (response_1['attributeScores']['TOXICITY']['spanScores'][0]['score']['value']\n",
    "                                    + response_2['attributeScores']['TOXICITY']['spanScores'][0]['score']['value'])/2\n",
    "            sample['Insult'] = (response_1['attributeScores']['INSULT']['spanScores'][0]['score']['value']/2\n",
    "                                    + response_2['attributeScores']['INSULT']['spanScores'][0]['score']['value'])\n",
    "            sample['Severe_Toxicity'] = (response_1['attributeScores']['SEVERE_TOXICITY']['spanScores'][0]['score']['value']\n",
    "                                    + response_2['attributeScores']['SEVERE_TOXICITY']['spanScores'][0]['score']['value'])/2\n",
    "            sample['Identity_Attack'] = (response_1['attributeScores']['IDENTITY_ATTACK']['spanScores'][0]['score']['value']\n",
    "                                    + response_2['attributeScores']['IDENTITY_ATTACK']['spanScores'][0]['score']['value'])/2\n",
    "            sample['Profanity'] = (response_1['attributeScores']['PROFANITY']['spanScores'][0]['score']['value']\n",
    "                                    + response_2['attributeScores']['PROFANITY']['spanScores'][0]['score']['value'])/2\n",
    "            \n",
    "            # update values\n",
    "            cols = ['Toxicity', 'Insult', 'Severe_Toxicity', 'Identity_Attack', 'Profanity']\n",
    "\n",
    "            for col in cols:\n",
    "                query = f''' UPDATE comments SET {col} = {sample[col]} WHERE id= '{sample['id']}' '''\n",
    "                conn.execute(query)\n",
    "                conn.commit()\n",
    "\n",
    "            \n",
    "        except:\n",
    "            try:\n",
    "                # store errors\n",
    "\n",
    "                error_df = pd.DataFrame([{'id': sample['id']}])\n",
    "                error_df.to_sql('errors_perspective', conn, if_exists='append')\n",
    "            except:\n",
    "                print(f\"Unmanageable error, Id: {sample['id']}\")\n",
    "\n",
    "    else: # if not, don't break it down\n",
    "        # handle exceptions\n",
    "        try:\n",
    "            # set headers\n",
    "            request_header = {\n",
    "        'comment': { 'text': sample_text},\n",
    "        'requestedAttributes': {'TOXICITY': {}, 'INSULT':{}, 'THREAT':{}, 'SEVERE_TOXICITY': {}, 'IDENTITY_ATTACK': {}, 'PROFANITY': {}}\n",
    "        }\n",
    "            # get response\n",
    "            response = client.comments().analyze(body=request_header).execute()\n",
    "\n",
    "            # adjust values\n",
    "            sample['Toxicity'] = response['attributeScores']['TOXICITY']['spanScores'][0]['score']['value']\n",
    "            sample['Insult'] = response['attributeScores']['INSULT']['spanScores'][0]['score']['value']\n",
    "            sample['Severe_Toxicity'] = response['attributeScores']['SEVERE_TOXICITY']['spanScores'][0]['score']['value']\n",
    "            sample['Identity_Attack'] = response['attributeScores']['IDENTITY_ATTACK']['spanScores'][0]['score']['value']\n",
    "            sample['Profanity'] = response['attributeScores']['PROFANITY']['spanScores'][0]['score']['value']\n",
    "\n",
    "            # update values\n",
    "            cols = ['Toxicity', 'Insult', 'Severe_Toxicity', 'Identity_Attack', 'Profanity']\n",
    "\n",
    "            for col in cols:\n",
    "                query = f''' UPDATE comments SET {col} = {sample[col]} WHERE id= '{sample['id']}' '''\n",
    "                conn.execute(query)\n",
    "                conn.commit()\n",
    "\n",
    "        except:\n",
    "            # store errors\n",
    "\n",
    "            error_df = pd.DataFrame([{'id': sample['id']}])\n",
    "            error_df.to_sql('errors_perspective', conn, if_exists='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95295f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(reddit):  \n",
    "    if reddit !=  '[removed]' and reddit is not None:\n",
    "\n",
    "        # removal of @name[mention]\n",
    "        regex_pat = re.compile(r\"@[\\w\\-]+\")\n",
    "        reddit_name = re.sub(regex_pat, '', reddit)\n",
    "        # removal of links[https://abc.com]\n",
    "        reddits = re.sub(\"(http|https)://[\\w\\-]+(\\.[\\w\\-]+)+\\S*\", '', reddit_name)\n",
    "\n",
    "        # removal of punctuations and numbers\n",
    "        punc_remove = re.sub(\"[^a-zA-Z]\", ' ', reddits)\n",
    "        \n",
    "        # removal of extra spaces\n",
    "        regex_pat = re.compile(r'\\s+')\n",
    "        reddit_space = re.sub(regex_pat, ' ', punc_remove)\n",
    "\n",
    "        # remove whitespace with a single space\n",
    "        newreddit=re.sub(r'\\s+', ' ', reddit_space)\n",
    "        # remove leading and trailing whitespace\n",
    "        newreddit=re.sub(r'^\\s+|\\s+?$', '', newreddit)\n",
    "        # removal of capitalization\n",
    "        reddit_lower = newreddit.lower()\n",
    "\n",
    "        return reddit_lower\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c6946dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and format the data\n",
    "\n",
    "path = './reddit_data_comments.db'\n",
    "conn = load_and_format(path)\n",
    "\n",
    "# parameters for Google Perspective\n",
    "api_key = \"AIzaSyBBhycm2m3xZTh95Tms50xaYUgXQ0_SoWM\"\n",
    "client = discovery.build(\n",
    "\"commentanalyzer\",\n",
    "\"v1alpha1\",\n",
    "developerKey=api_key,\n",
    "discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
    "static_discovery=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d8bc2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM comments WHERE body is NOT '' AND body is not NULL AND Toxicity is NULL;\"\n",
    "reddit_df = pd.read_sql_query(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "042da256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'author', 'body', 'created_utc', 'is_submitter', 'link_id',\n",
       "       'score', 'subreddit', 'subreddit_id', 'Toxicity', 'Insult',\n",
       "       'Severe_Toxicity', 'Identity_Attack', 'Profanity'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a356f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean it\n",
    "count = 0\n",
    "SELFTEXT_INDEX = 2\n",
    "exception_log = open('exception_process_list', 'w')\n",
    "clean_reddit_df = pd.DataFrame(columns = ['id', 'author', 'body', 'created_utc', 'is_submitter', 'link_id',\n",
    "    'score', 'subreddit', 'subreddit_id', 'Toxicity', 'Insult',\n",
    "    'Severe_Toxicity', 'Identity_Attack', 'Profanity'])\n",
    "\n",
    "for i in range(10):\n",
    "    processed_text = \"\"\n",
    "    text = reddit_df.iloc[i]['body']\n",
    "    try:\n",
    "        processed_text = preprocess(text)\n",
    "    except:\n",
    "        exception_log.write(str(count) +',' + text + '\\n')\n",
    "    else:\n",
    "        if processed_text != '':\n",
    "            temp = np.array(reddit_df.iloc[count])\n",
    "            temp[SELFTEXT_INDEX] = processed_text\n",
    "            clean_reddit_df.loc[len(clean_reddit_df)] = temp\n",
    "    finally:\n",
    "        count +=1 \n",
    "    if count % 500 == 0:\n",
    "        print(count)\n",
    "exception_log.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c6bb9ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "database is locked",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_180386/699164105.py\", line 84, in parse_googlePerspective\n    conn.execute(query)\nsqlite3.OperationalError: database is locked\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 51, in starmapstar\n    return list(itertools.starmap(args[0], args[1]))\n  File \"/tmp/ipykernel_180386/699164105.py\", line 91, in parse_googlePerspective\n    error_df.to_sql('errors_perspective', conn, if_exists='append')\n  File \"/home/marszzibros/.local/lib/python3.10/site-packages/pandas/core/generic.py\", line 2987, in to_sql\n    return sql.to_sql(\n  File \"/home/marszzibros/.local/lib/python3.10/site-packages/pandas/io/sql.py\", line 695, in to_sql\n    return pandas_sql.to_sql(\n  File \"/home/marszzibros/.local/lib/python3.10/site-packages/pandas/io/sql.py\", line 2188, in to_sql\n    return table.insert(chunksize, method)\n  File \"/home/marszzibros/.local/lib/python3.10/site-packages/pandas/io/sql.py\", line 946, in insert\n    num_inserted = exec_insert(conn, keys, chunk_iter)\n  File \"/home/marszzibros/.local/lib/python3.10/site-packages/pandas/io/sql.py\", line 1894, in _execute_insert\n    conn.executemany(self.insert_statement(num_rows=1), data_list)\nsqlite3.OperationalError: database is locked\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# send batch requests by multiprocessing\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mwith\u001b[39;00m Pool(processes\u001b[39m=\u001b[39mcpu_count()) \u001b[39mas\u001b[39;00m pl:\n\u001b[0;32m----> 3\u001b[0m     pl\u001b[39m.\u001b[39;49mstarmap(parse_googlePerspective, [(path, client, clean_reddit_df\u001b[39m.\u001b[39;49mloc[i]) \u001b[39mfor\u001b[39;49;00m i \u001b[39min\u001b[39;49;00m \u001b[39mrange\u001b[39;49m(\u001b[39mlen\u001b[39;49m(clean_reddit_df))])\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py:375\u001b[0m, in \u001b[0;36mPool.starmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstarmap\u001b[39m(\u001b[39mself\u001b[39m, func, iterable, chunksize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    370\u001b[0m     \u001b[39m'''\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[39m    Like `map()` method but the elements of the `iterable` are expected to\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[39m    be iterables as well and will be unpacked as arguments. Hence\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[39m    `func` and (a, b) becomes func(a, b).\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m--> 375\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_async(func, iterable, starmapstar, chunksize)\u001b[39m.\u001b[39;49mget()\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py:774\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    772\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n\u001b[1;32m    773\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 774\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n",
      "\u001b[0;31mOperationalError\u001b[0m: database is locked"
     ]
    }
   ],
   "source": [
    "\n",
    "# send batch requests by multiprocessing\n",
    "with Pool(processes=cpu_count()) as pl:\n",
    "    pl.starmap(parse_googlePerspective, [(path, client, clean_reddit_df.loc[i]) for i in range(len(clean_reddit_df))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67c1ea8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
