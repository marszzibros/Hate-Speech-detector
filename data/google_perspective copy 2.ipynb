{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Hate Speech Detector</h1>\n",
    "<h3>CS/CSYS/STAT 287 Data Science</h3>\n",
    "<h4>Data retrival</h4>\n",
    "<h4>Aviral Chawla, Daniel Orem, Jay Hwasung Jung, Shunsuke Miyazato</h4>\n",
    "---------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading in modules\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle \n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "specify file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create file path\n",
    "db_path = \"./reddit_data_comments.db\"\n",
    "bn_path = \"./model/BernouN/BernouN\" + str(np.random.randint(10, size = 1)[0]) + \".sav\"\n",
    "mb_path = \"./model/MultiNB/MultiNB\" + str(np.random.randint(10, size = 1)[0]) + \".sav\"\n",
    "sv_path = \"./model/SVM/SVM\" + str(np.random.randint(10, size = 1)[0]) + \".sav\"\n",
    "\n",
    "# Create a SQL connection\n",
    "con = sqlite3.connect(db_path)\n",
    "cursor = con.cursor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              id           author  \\\n",
      "0        dy9bf8o      theothermod   \n",
      "1        dy9bh0f         lpoop789   \n",
      "2        dy9bi4e      theothermod   \n",
      "3        dy9cqc9  JackFisherBooks   \n",
      "4        dy9e5cc      thrway_1000   \n",
      "...          ...              ...   \n",
      "5717238  hica7f7    AutoModerator   \n",
      "5717239  hica7g2    AutoModerator   \n",
      "5717240  hjuaa29           dmemed   \n",
      "5717241  hu3hzpu        steve_tom   \n",
      "5717242  hwtzdi0        [deleted]   \n",
      "\n",
      "                                                      body  created_utc  \\\n",
      "0        The title of your post does not say enough abo...   1525163899   \n",
      "1                                                       Ok   1525164011   \n",
      "2        Describe what it's about and post it again. \"M...   1525164082   \n",
      "3        There's a lot of media criticism these days ab...   1525166916   \n",
      "4        Vice, when you need stupid people to feed you ...   1525170375   \n",
      "...                                                    ...          ...   \n",
      "5717238  [Glory to Stalin!](https://www.youtube.com/wat...   1635393898   \n",
      "5717239  Thanks for signing up to AOC facts! You will n...   1635393899   \n",
      "5717240  Bit of an older comment, so sorry for the repl...   1636399971   \n",
      "5717241  Apparently, [Genghis Khan was a communist](htt...   1643075470   \n",
      "5717242                                          [removed]   1644790053   \n",
      "\n",
      "         is_submitter    link_id  score   subreddit subreddit_id  Toxicity  \\\n",
      "0                 0.0  t3_8g5zag      1  MensRights     t5_2qhk3  0.093515   \n",
      "1                 0.0  t3_8g5zag      1  MensRights     t5_2qhk3  0.015016   \n",
      "2                 0.0  t3_8g5zag      1  MensRights     t5_2qhk3  0.203122   \n",
      "3                 1.0  t3_8g7568      6  MensRights     t5_2qhk3       NaN   \n",
      "4                 0.0  t3_8g780h     14  MensRights     t5_2qhk3  0.840319   \n",
      "...               ...        ...    ...         ...          ...       ...   \n",
      "5717238           0.0  t3_qhb5c0      1   GenZedong    t5_1vnbn1       NaN   \n",
      "5717239           0.0  t3_qhb5c0      1   GenZedong    t5_1vnbn1       NaN   \n",
      "5717240           0.0  t3_q1c5g6      2   GenZedong    t5_1vnbn1       NaN   \n",
      "5717241           0.0  t3_q1c5g6      1   GenZedong    t5_1vnbn1       NaN   \n",
      "5717242           0.0  t3_q1c5g6      1   GenZedong    t5_1vnbn1       NaN   \n",
      "\n",
      "           Insult  Severe_Toxicity  Identity_Attack  Profanity  \n",
      "0        0.031367         0.002918         0.006216   0.041913  \n",
      "1        0.008387         0.001593         0.003052   0.016685  \n",
      "2        0.084840         0.004101         0.026208   0.052935  \n",
      "3             NaN              NaN              NaN        NaN  \n",
      "4        0.777521         0.169603         0.092455   0.444313  \n",
      "...           ...              ...              ...        ...  \n",
      "5717238       NaN              NaN              NaN        NaN  \n",
      "5717239       NaN              NaN              NaN        NaN  \n",
      "5717240       NaN              NaN              NaN        NaN  \n",
      "5717241       NaN              NaN              NaN        NaN  \n",
      "5717242       NaN              NaN              NaN        NaN  \n",
      "\n",
      "[5717243 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "# get list from database\n",
    "query = \"SELECT * FROM comments;\"\n",
    "\n",
    "reddit_df = pd.read_sql_query(query, con)\n",
    "print(reddit_df)\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'author', 'body', 'created_utc', 'is_submitter', 'link_id',\n",
      "       'score', 'subreddit', 'subreddit_id', 'Toxicity', 'Insult',\n",
      "       'Severe_Toxicity', 'Identity_Attack', 'Profanity'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(reddit_df.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocess(reddit):  \n",
    "    if reddit !=  '[removed]' and reddit is not None:\n",
    "\n",
    "        # removal of @name[mention]\n",
    "        regex_pat = re.compile(r\"@[\\w\\-]+\")\n",
    "        reddit_name = re.sub(regex_pat, '', reddit)\n",
    "        # removal of links[https://abc.com]\n",
    "        reddits = re.sub(\"(http|https)://[\\w\\-]+(\\.[\\w\\-]+)+\\S*\", '', reddit_name)\n",
    "\n",
    "        # removal of punctuations and numbers\n",
    "        punc_remove = re.sub(\"[^a-zA-Z]\", ' ', reddits)\n",
    "        \n",
    "        # removal of extra spaces\n",
    "        regex_pat = re.compile(r'\\s+')\n",
    "        reddit_space = re.sub(regex_pat, ' ', punc_remove)\n",
    "\n",
    "        # remove whitespace with a single space\n",
    "        newreddit=re.sub(r'\\s+', ' ', reddit_space)\n",
    "        # remove leading and trailing whitespace\n",
    "        newreddit=re.sub(r'^\\s+|\\s+?$', '', newreddit)\n",
    "        # removal of capitalization\n",
    "        reddit_lower = newreddit.lower()\n",
    "\n",
    "        return reddit_lower\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean it\n",
    "count = 0\n",
    "SELFTEXT_INDEX = 2\n",
    "exception_log = open('exception_process_list_comments', 'w')\n",
    "clean_reddit_df = pd.DataFrame(columns = ['id', 'author', 'body', 'created_utc', 'is_submitter', 'link_id',\n",
    "       'score', 'subreddit', 'subreddit_id', 'Toxicity', 'Insult',\n",
    "       'Severe_Toxicity', 'Identity_Attack', 'Profanity'])\n",
    "for text in reddit_df.iloc[:10]['body']:\n",
    "    processed_text = \"\"\n",
    "    try:\n",
    "        processed_text = preprocess(text)\n",
    "    except:\n",
    "        exception_log.write(str(count) +',' + text + '\\n')\n",
    "    else:\n",
    "        if processed_text != '':\n",
    "            temp = np.array(reddit_df.iloc[count])\n",
    "            temp[SELFTEXT_INDEX] = processed_text\n",
    "            clean_reddit_df.loc[len(clean_reddit_df)] = temp\n",
    "    finally:\n",
    "        count +=1 \n",
    "        if count % 10000 == 0:\n",
    "            print(count)\n",
    "exception_log.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "print(len(clean_reddit_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient import discovery\n",
    "import json\n",
    "\n",
    "API_KEY = 'AIzaSyBBhycm2m3xZTh95Tms50xaYUgXQ0_SoWM'\n",
    "\n",
    "client = discovery.build(\n",
    "  \"commentanalyzer\",\n",
    "  \"v1alpha1\",\n",
    "  developerKey=API_KEY,\n",
    "  discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
    "  static_discovery=False,\n",
    ")\n",
    "\n",
    "def google_perspective(text):\n",
    "    \n",
    "    sample = {'Toxicity': -1, 'Insult':-1, 'THREAT': -1, 'Severe_Toxicity': -1, 'Identity_Attack': -1, 'Profanity': -1}\n",
    "    exception_log = open('exception_process_list_google_comments.txt', 'a')\n",
    "    # if it is larger, then break it down\n",
    "    if len(text.encode('utf-8')) > 20480:\n",
    "        try:\n",
    "            # break it in two:\n",
    "\n",
    "            # set two headers\n",
    "            response_header_1 = {\n",
    "    'comment': { 'text': text[:20000]},\n",
    "    'requestedAttributes': {'TOXICITY': {}, 'INSULT':{}, 'THREAT':{}, 'SEVERE_TOXICITY': {}, 'IDENTITY_ATTACK': {}, 'PROFANITY': {}}\n",
    "    }       \n",
    "            response_header_2 = {\n",
    "    'comment': { 'text': text[20000:]},\n",
    "    'requestedAttributes': {'TOXICITY': {}, 'INSULT':{}, 'THREAT':{}, 'SEVERE_TOXICITY': {}, 'IDENTITY_ATTACK': {}, 'PROFANITY': {}}\n",
    "    }\n",
    "            \n",
    "            # get two responses\n",
    "\n",
    "            response_1 = client.comments().analyze(body=response_header_1).execute()\n",
    "            response_2 = client.comments().analyze(body=response_header_2).execute()\n",
    "            \n",
    "            # add their average response\n",
    "            sample['Toxicity'] = (response_1['attributeScores']['TOXICITY']['spanScores'][0]['score']['value']\n",
    "                                    + response_2['attributeScores']['TOXICITY']['spanScores'][0]['score']['value'])/2\n",
    "            sample['Insult'] = (response_1['attributeScores']['INSULT']['spanScores'][0]['score']['value']/2\n",
    "                                    + response_2['attributeScores']['INSULT']['spanScores'][0]['score']['value'])\n",
    "            sample['Severe_Toxicity'] = (response_1['attributeScores']['SEVERE_TOXICITY']['spanScores'][0]['score']['value']\n",
    "                                    + response_2['attributeScores']['SEVERE_TOXICITY']['spanScores'][0]['score']['value'])/2\n",
    "            sample['Identity_Attack'] = (response_1['attributeScores']['IDENTITY_ATTACK']['spanScores'][0]['score']['value']\n",
    "                                    + response_2['attributeScores']['IDENTITY_ATTACK']['spanScores'][0]['score']['value'])/2\n",
    "            sample['Profanity'] = (response_1['attributeScores']['PROFANITY']['spanScores'][0]['score']['value']\n",
    "                                    + response_2['attributeScores']['PROFANITY']['spanScores'][0]['score']['value'])/2\n",
    "        except:\n",
    "            exception_log.write(text + '\\n')\n",
    "        finally:\n",
    "            exception_log.close()\n",
    "            return sample\n",
    "\n",
    "    else: # if not, don't break it down\n",
    "        # handle exceptions\n",
    "        try:\n",
    "            # set headers\n",
    "            request_header = {\n",
    "        'comment': { 'text': text},\n",
    "        'requestedAttributes': {'TOXICITY': {}, 'INSULT':{}, 'THREAT':{}, 'SEVERE_TOXICITY': {}, 'IDENTITY_ATTACK': {}, 'PROFANITY': {}}\n",
    "        }\n",
    "            # get response\n",
    "            response = client.comments().analyze(body=request_header).execute()\n",
    "\n",
    "            # adjust values\n",
    "            sample['Toxicity'] = response['attributeScores']['TOXICITY']['spanScores'][0]['score']['value']\n",
    "            sample['Insult'] = response['attributeScores']['INSULT']['spanScores'][0]['score']['value']\n",
    "            sample['Severe_Toxicity'] = response['attributeScores']['SEVERE_TOXICITY']['spanScores'][0]['score']['value']\n",
    "            sample['Identity_Attack'] = response['attributeScores']['IDENTITY_ATTACK']['spanScores'][0]['score']['value']\n",
    "            sample['Profanity'] = response['attributeScores']['PROFANITY']['spanScores'][0]['score']['value']\n",
    "        except:\n",
    "            exception_log.write(text + '\\n')\n",
    "        finally:\n",
    "            exception_log.close()\n",
    "            return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient import discovery\n",
    "import json\n",
    "\n",
    "API_KEY = 'AIzaSyBBhycm2m3xZTh95Tms50xaYUgXQ0_SoWM'\n",
    "\n",
    "client = discovery.build(\n",
    "  \"commentanalyzer\",\n",
    "  \"v1alpha1\",\n",
    "  developerKey=API_KEY,\n",
    "  discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
    "  static_discovery=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "URL = \"https://jjung2.w3.uvm.edu/hatespeech/api/write.php\"\n",
    "\n",
    "exception_log = open('exception_request.txt', 'w')\n",
    "count = 0\n",
    "for text in clean_reddit_df['body']:\n",
    "    sample = google_perspective(text)\n",
    "    \n",
    "    if sample['Toxicity'] != -1:\n",
    "        # dictionaries for testing\n",
    "        rest_request = {'request' : 'comments',\n",
    "        'id' : clean_reddit_df.iloc[count]['id'],\n",
    "        'author' : clean_reddit_df.iloc[count]['author'],\n",
    "        'body' : clean_reddit_df.iloc[count]['body'][:20000],\n",
    "        'created_utc' : clean_reddit_df.iloc[count]['created_utc'],\n",
    "        'is_submitter' : clean_reddit_df.iloc[count]['is_submitter'],\n",
    "        'link_id' : clean_reddit_df.iloc[count]['link_id'],\n",
    "        'score' : clean_reddit_df.iloc[count]['score'],\n",
    "        'subreddit' : clean_reddit_df.iloc[count]['subreddit'],\n",
    "        'subreddit_id' : clean_reddit_df.iloc[count]['subreddit_id'],\n",
    "        'Toxicity' : sample['Toxicity'],\n",
    "        'Insult' : sample['Insult'],\n",
    "        'Severe_Toxicity' : sample['Severe_Toxicity'],\n",
    "        'Identity_Attack' : sample['Identity_Attack'],\n",
    "        'Profanity' : sample['Profanity']}\n",
    "\n",
    "        try:\n",
    "            x = requests.post(URL, data = rest_request)\n",
    "        except:\n",
    "            exception_log.write(text + '\\n')\n",
    "    count +=1\n",
    "exception_log.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOXICITY:  0.17891699\n",
      "INSULT  :  0.027841117\n",
      "THREAT  :  0.12996206\n"
     ]
    }
   ],
   "source": [
    "print(\"TOXICITY: \", response['attributeScores']['TOXICITY']['spanScores'][0]['score']['value'])\n",
    "print(\"INSULT  : \", response['attributeScores']['INSULT']['spanScores'][0]['score']['value'])\n",
    "print(\"THREAT  : \", response['attributeScores']['THREAT']['spanScores'][0]['score']['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mjwwlr\n"
     ]
    }
   ],
   "source": [
    "print(summary_hate_speech.iloc[len(summary_hate_speech) - 1]['id'])\n",
    "summary_hate_speech.to_csv(index=False)\n",
    "summary_hate_speech.to_csv('./out_2.csv')  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
