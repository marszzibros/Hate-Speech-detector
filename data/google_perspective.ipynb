{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Hate Speech Detector</h1>\n",
    "<h3>CS/CSYS/STAT 287 Data Science</h3>\n",
    "<h4>Data retrival</h4>\n",
    "<h4>Aviral Chawla, Daniel Orem, Jay Hwasung Jung, Shunsuke Miyazato</h4>\n",
    "---------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading in modules\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle \n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "specify file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create file path\n",
    "db_path = \"./reddit_data.db\"\n",
    "bn_path = \"./model/BernouN/BernouN\" + str(np.random.randint(10, size = 1)[0]) + \".sav\"\n",
    "mb_path = \"./model/MultiNB/MultiNB\" + str(np.random.randint(10, size = 1)[0]) + \".sav\"\n",
    "sv_path = \"./model/SVM/SVM\" + str(np.random.randint(10, size = 1)[0]) + \".sav\"\n",
    "\n",
    "# Create a SQL connection\n",
    "con = sqlite3.connect(db_path)\n",
    "cursor = con.cursor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            id            author  created_utc  \\\n",
      "0       8g5sxr     JohnKimble111   1525148826   \n",
      "1       8g5zag          lpoop789   1525150942   \n",
      "2       8g644k        danielmd92   1525152596   \n",
      "3       8g6ynw          lpoop789   1525164358   \n",
      "4       8g7568   JackFisherBooks   1525166725   \n",
      "...        ...               ...          ...   \n",
      "425896  tejovt        jomama1322   1647330439   \n",
      "425897  tekm6q        jomama1322   1647334652   \n",
      "425898  teup3f       tahaabid262   1647365993   \n",
      "425899  tfttty  Ok-Industry-6065   1647466595   \n",
      "425900  tipxm2         MiojoComN   1647796778   \n",
      "\n",
      "                                                full_link  is_self  is_video  \\\n",
      "0       https://www.reddit.com/r/MensRights/comments/8...        0         0   \n",
      "1       https://www.reddit.com/r/MensRights/comments/8...        0         0   \n",
      "2       https://www.reddit.com/r/MensRights/comments/8...        0         0   \n",
      "3       https://www.reddit.com/r/MensRights/comments/8...        0         0   \n",
      "4       https://www.reddit.com/r/MensRights/comments/8...        0         0   \n",
      "...                                                   ...      ...       ...   \n",
      "425896  https://www.reddit.com/r/darkmememe/comments/t...        0         1   \n",
      "425897  https://www.reddit.com/r/darkmememe/comments/t...        0         1   \n",
      "425898  https://www.reddit.com/r/darkmememe/comments/t...        0         0   \n",
      "425899  https://www.reddit.com/r/darkmememe/comments/t...        0         1   \n",
      "425900  https://www.reddit.com/r/darkmememe/comments/t...        0         0   \n",
      "\n",
      "        num_comments  num_crossposts  over_18  score selftext   subreddit  \\\n",
      "0                  4               0        0     40           MensRights   \n",
      "1                  3               0        0      1           MensRights   \n",
      "2                 32               0        0    133           MensRights   \n",
      "3                 36               0        0    357           MensRights   \n",
      "4                  9               0        0     20           MensRights   \n",
      "...              ...             ...      ...    ...      ...         ...   \n",
      "425896            12               0        0      1           darkmememe   \n",
      "425897             6               0        0      1           darkmememe   \n",
      "425898             3               0        0      1           darkmememe   \n",
      "425899            16               0        1      1           darkmememe   \n",
      "425900             2               0        0      1           darkmememe   \n",
      "\n",
      "       subreddit_id                                              title  \\\n",
      "0          t5_2qhk3  Man Claims He Saved 25% on Car Insurance By Ch...   \n",
      "1          t5_2qhk3                                                Men   \n",
      "2          t5_2qhk3  Been paying child support to my rapist for 16 ...   \n",
      "3          t5_2qhk3                     Men Deserve love and affection   \n",
      "4          t5_2qhk3  Profiles In Noble Masculinity: Joel From â€œThe ...   \n",
      "...             ...                                                ...   \n",
      "425896    t5_3qsh5e                       Joe Biden finally got caught   \n",
      "425897    t5_3qsh5e                 resurrect him or else you're fired   \n",
      "425898    t5_3qsh5e                                       Caught lying   \n",
      "425899    t5_3qsh5e  me when i go to a random house and find my new...   \n",
      "425900    t5_3qsh5e                                      First attempt   \n",
      "\n",
      "                                                      url  Toxicity  Insult  \\\n",
      "0       https://notablelife.com/canadian-man-claims-he...       NaN     NaN   \n",
      "1                     https://i.redd.it/d9awzcv9f6v01.jpg       NaN     NaN   \n",
      "2       https://www.reddit.com/r/SupportforMen/comment...       NaN     NaN   \n",
      "3                     https://i.redd.it/q7f3hu06j7v01.jpg       NaN     NaN   \n",
      "4       https://jackfisherbooks.com/2018/05/01/profile...       NaN     NaN   \n",
      "...                                                   ...       ...     ...   \n",
      "425896                    https://v.redd.it/fwmy5qmm5in81       NaN     NaN   \n",
      "425897                    https://v.redd.it/k0j3lc06iin81       NaN     NaN   \n",
      "425898                https://i.redd.it/y1665z3c3ln81.jpg       NaN     NaN   \n",
      "425899                    https://v.redd.it/y7s0wyqhetn81       NaN     NaN   \n",
      "425900                https://i.redd.it/f9wscb2boko81.jpg       NaN     NaN   \n",
      "\n",
      "        Severe_Toxicity  Identity_Attack  Profanity  \n",
      "0                   NaN              NaN        NaN  \n",
      "1                   NaN              NaN        NaN  \n",
      "2                   NaN              NaN        NaN  \n",
      "3                   NaN              NaN        NaN  \n",
      "4                   NaN              NaN        NaN  \n",
      "...                 ...              ...        ...  \n",
      "425896              NaN              NaN        NaN  \n",
      "425897              NaN              NaN        NaN  \n",
      "425898              NaN              NaN        NaN  \n",
      "425899              NaN              NaN        NaN  \n",
      "425900              NaN              NaN        NaN  \n",
      "\n",
      "[425901 rows x 20 columns]\n"
     ]
    }
   ],
   "source": [
    "# get list from database\n",
    "query = \"SELECT * FROM submissions;\"\n",
    "\n",
    "reddit_df = pd.read_sql_query(query, con)\n",
    "print(reddit_df)\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'author', 'created_utc', 'full_link', 'is_self', 'is_video',\n",
      "       'num_comments', 'num_crossposts', 'over_18', 'score', 'selftext',\n",
      "       'subreddit', 'subreddit_id', 'title', 'url', 'Toxicity', 'Insult',\n",
      "       'Severe_Toxicity', 'Identity_Attack', 'Profanity'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(reddit_df.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(reddit):  \n",
    "    if reddit !=  '[removed]' and reddit is not None:\n",
    "\n",
    "        # removal of @name[mention]\n",
    "        regex_pat = re.compile(r\"@[\\w\\-]+\")\n",
    "        reddit_name = re.sub(regex_pat, '', reddit)\n",
    "        # removal of links[https://abc.com]\n",
    "        reddits = re.sub(\"(http|https)://[\\w\\-]+(\\.[\\w\\-]+)+\\S*\", '', reddit_name)\n",
    "\n",
    "        # removal of punctuations and numbers\n",
    "        punc_remove = re.sub(\"[^a-zA-Z]\", ' ', reddits)\n",
    "        \n",
    "        # removal of extra spaces\n",
    "        regex_pat = re.compile(r'\\s+')\n",
    "        reddit_space = re.sub(regex_pat, ' ', punc_remove)\n",
    "\n",
    "        # remove whitespace with a single space\n",
    "        newreddit=re.sub(r'\\s+', ' ', reddit_space)\n",
    "        # remove leading and trailing whitespace\n",
    "        newreddit=re.sub(r'^\\s+|\\s+?$', '', newreddit)\n",
    "        # removal of capitalization\n",
    "        reddit_lower = newreddit.lower()\n",
    "\n",
    "        return reddit_lower\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After scrolling through both r/feminism and r/mensrights, the main problem I see is that most of the posts are not statistics but individual events. As someone who likes to make decisions based on statistics, neither of these subreddits has furthered my view of gender equality. The reason why I do not like individual events is because they are usually outliers specifically picked to make a point. They also usually do not accurately describe the problem.\n",
      "the simplest response is so female victims of dv are taken seriously because of that same patriarchy is feminism using patriarchal attitudes and hijacking the patriarchy to serve women s interests\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(reddit_df.loc[511]['selftext'])\n",
    "print(preprocess(reddit_df[reddit_df['is_self'] == 1]['selftext'].loc[6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "SELFTEXT_INDEX = 10\n",
    "exception_log = open('exception_process_list', 'w')\n",
    "clean_reddit_df = pd.DataFrame(columns = ['id', 'author', 'created_utc', 'full_link', 'is_self', 'is_video',\n",
    "       'num_comments', 'num_crossposts', 'over_18', 'score', 'selftext',\n",
    "       'subreddit', 'subreddit_id', 'title', 'url'])\n",
    "del reddit_df[\"Toxicity\"]\n",
    "del reddit_df[\"Insult\"]\n",
    "del reddit_df[\"Severe_Toxicity\"]\n",
    "del reddit_df[\"Identity_Attack\"]\n",
    "del reddit_df[\"Profanity\"]\n",
    "for text in reddit_df[reddit_df['is_self'] == 1]['selftext']:\n",
    "    processed_text = \"\"\n",
    "    try:\n",
    "        processed_text = preprocess(text)\n",
    "    except:\n",
    "        exception_log.write(str(count) +',' + text + '\\n')\n",
    "    else:\n",
    "        if processed_text != '':\n",
    "            temp = np.array(reddit_df.iloc[count])\n",
    "            temp[SELFTEXT_INDEX] = processed_text\n",
    "            clean_reddit_df.loc[len(clean_reddit_df)] = temp\n",
    "    finally:\n",
    "        count +=1 \n",
    "exception_log.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                                         \n",
      "1                                                         \n",
      "2                                                         \n",
      "3                                                         \n",
      "4                                                         \n",
      "                               ...                        \n",
      "24405    Several years ago, before I settled down and s...\n",
      "24406    Anyone wanna message me about wanting to dress...\n",
      "24407                                                     \n",
      "24408                                                     \n",
      "24409    \" THE statue of Suffragette leader Emmeline Pa...\n",
      "Name: selftext, Length: 24410, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(len(clean_reddit_df['selftext']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient import discovery\n",
    "import json\n",
    "\n",
    "API_KEY = 'AIzaSyBBhycm2m3xZTh95Tms50xaYUgXQ0_SoWM'\n",
    "\n",
    "client = discovery.build(\n",
    "  \"commentanalyzer\",\n",
    "  \"v1alpha1\",\n",
    "  developerKey=API_KEY,\n",
    "  discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
    "  static_discovery=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n"
     ]
    }
   ],
   "source": [
    "\n",
    "exception_log = open('exception_googleperspective_list', 'w')\n",
    "summary_hate_speech = pd.DataFrame(columns= ['text', 'id' ,'author', 'created_utc', 'score', 'subreddit', 'TOXICITY', 'INSULT', 'THREAT'])\n",
    "\n",
    "for i in range(len(clean_reddit_df)):\n",
    "    try:\n",
    "        analyze_request = {\n",
    "            'comment': { 'text': clean_reddit_df.iloc[i]['selftext']},\n",
    "            'requestedAttributes': {'TOXICITY': {}, 'INSULT':{}, 'THREAT':{}}\n",
    "            }\n",
    "        response = client.comments().analyze(body=analyze_request).execute()\n",
    "        summary_hate_speech.loc[len(summary_hate_speech.index)] = [\n",
    "            clean_reddit_df.iloc[i]['selftext'],\n",
    "            clean_reddit_df.iloc[i]['id'],\n",
    "            clean_reddit_df.iloc[i]['author'],\n",
    "            clean_reddit_df.iloc[i]['created_utc'],\n",
    "            clean_reddit_df.iloc[i]['score'],\n",
    "            clean_reddit_df.iloc[i]['subreddit'],\n",
    "            response['attributeScores']['TOXICITY']['spanScores'][0]['score']['value'],\n",
    "            response['attributeScores']['INSULT']['spanScores'][0]['score']['value'],\n",
    "            response['attributeScores']['THREAT']['spanScores'][0]['score']['value']]\n",
    "    except:\n",
    "        exception_log.write(str(i) + ',' + clean_reddit_df.iloc[i]['selftext'] + '\\n')\n",
    "    else:\n",
    "        if i % 100 == 0:\n",
    "            print(i)\n",
    "summary_hate_speech.to_csv(index=False)\n",
    "exception_log.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TOXICITY: \", response['attributeScores']['TOXICITY']['spanScores'][0]['score']['value'])\n",
    "print(\"INSULT  : \", response['attributeScores']['INSULT']['spanScores'][0]['score']['value'])\n",
    "print(\"THREAT  : \", response['attributeScores']['THREAT']['spanScores'][0]['score']['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary_hate_speech.iloc[len(summary_hate_speech) - 1]['id'])\n",
    "summary_hate_speech.to_csv(index=False)\n",
    "summary_hate_speech.to_csv('./out_2.csv')  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
